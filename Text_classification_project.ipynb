{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os   # for traversing the files in directory\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re     # regular expressions for splitting the data into seperate strings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report , confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function generates the vocabulary by reaDING THE STRINGS IN ALL 16000 files and comparing them with \n",
    "                        #the list of stopwords, rest 4000 files are for testing and they are not used in building vocab\n",
    "def build_vocabulary(rootdir,stopwords):      \n",
    "    vocab={}\n",
    "    for dirname,subdir,filename in os.walk(rootdir):\n",
    "        cnt=0\n",
    "        for file in filename:\n",
    "            if cnt>=800:      # for each class we are taking 800 files for training\n",
    "                continue\n",
    "            filepath= os.path.join(dirname,file)\n",
    "            with open(filepath,'r') as f:\n",
    "                #for j in range(12):     # skipping the first 12 lines of each file as it contains useless info.\n",
    "                 #   f.readline() \n",
    "                filedata = f.read()\n",
    "                filedata = filedata.lower()\n",
    "                wordlist = re.split(r\"\\W|\\d\",filedata)   # splitting the data into seperate strings\n",
    "                counter=Counter(wordlist)\n",
    "                common_words=list(set(wordlist)-set(stopwords))\n",
    "                for word in common_words:\n",
    "                    freq=counter[word]          # adding word in vocab if it's not in stopwords\n",
    "                    if word in vocab:\n",
    "                        vocab[word]+=freq\n",
    "                    else:\n",
    "                        vocab[word]=freq\n",
    "            cnt+=1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.4022650718689\n"
     ]
    }
   ],
   "source": [
    "# rootdir is the path where the files are in my pc  \n",
    "rootdir='E:/machine_learning/attachments/csv/text_classification_data/20_newsgroups/20_newsgroups'\n",
    "stopwords = list([\"\",\" \",\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"])\n",
    "#print(stopwords)\n",
    "start=time.time()\n",
    "vocabulary= build_vocabulary(rootdir,stopwords)  # generate the vocabulary\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16411733627319336\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109221"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting the vocabulary on the basis of frequency of words\n",
    "vocab={}\n",
    "def cmp(t):\n",
    "    return t[1]\n",
    "start=time.time()\n",
    "for k,v in sorted(vocabulary.items(),key = cmp,reverse = True):\n",
    "    vocab[k]= v\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(vocab==vocabulary)\n",
    "#print(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJJqMzQkSbUZGpKk2v4/x+mgwidoq7MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x295b57dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x295a449fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting freq vs number of words with freq greater than or equal to that freq\n",
    "y_axis=vocab.values()\n",
    "x_axis=[]\n",
    "cnt=1\n",
    "for freq in y_axis:\n",
    "    x_axis.append(cnt)\n",
    "    cnt+=1\n",
    "plt.plot(x_axis,y_axis)\n",
    "plt.grid()\n",
    "#plt.axis([8000,100000,0,50])\n",
    "plt.show()\n",
    "plt.plot(x_axis,y_axis)\n",
    "plt.grid()\n",
    "plt.axis([0,40,0,80000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9989"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking only the top 10000 words from the vocabulary\n",
    "cnt=0\n",
    "vocabulary={}\n",
    "for k,v in vocab.items():\n",
    "    if cnt<=10:\n",
    "        cnt+=1\n",
    "        continue\n",
    "    if cnt>=10000:\n",
    "        break\n",
    "    vocabulary[k]= v\n",
    "    cnt+=1\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates the training and testing data in form of arrays\n",
    "def gen_train_test(rootdir,vocabulary,class_list):\n",
    "    x_train , x_test , y_train , y_test = [],[],[],[]\n",
    "    vocab_idx_dict={}\n",
    "    i=0\n",
    "    for key in vocabulary.keys():\n",
    "        vocab_idx_dict[key]=i\n",
    "        i+=1\n",
    "    for dirname,subdir,filename in os.walk(rootdir):\n",
    "        cnt=0\n",
    "        class_name=''\n",
    "        for c_name in class_list:\n",
    "            if c_name in dirname:\n",
    "                class_name=c_name\n",
    "        for file in filename:\n",
    "            filepath=os.path.join(dirname,file)\n",
    "            x=np.zeros(len(vocabulary))\n",
    "            with open(filepath,'r') as f:\n",
    "                #for j in range(12):\n",
    "                 #   f.readline()\n",
    "                filedata=f.read()\n",
    "                filedata=filedata.lower()\n",
    "                wordlist = re.split(r\"\\W|\\d\",filedata)\n",
    "                counter=Counter(wordlist)\n",
    "                common_words=list(set(vocabulary)&set(wordlist))\n",
    "                for word in common_words:   # for each file we are adding a row with the freq of each word in vocabulary\n",
    "                    freq=counter[word]           # in that particular file\n",
    "                    idx=vocab_idx_dict[word]\n",
    "                    x[idx]=freq\n",
    "            if cnt<800:                    # 800 files of each class are added in the training data, rest 200 in testing\n",
    "                x_train.append(x)\n",
    "                y_train.append(class_name)\n",
    "            else:\n",
    "                x_test.append(x)\n",
    "                y_test.append(class_name)\n",
    "            cnt+=1\n",
    "    return x_train,x_test,y_train,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.50481295585632\n"
     ]
    }
   ],
   "source": [
    "# class_list contains the name of all 20 classes\n",
    "class_list=os.listdir(rootdir)\n",
    "start=time.time()\n",
    "x_train,x_test,y_train,y_test=gen_train_test(rootdir,vocabulary,class_list) # generates the training and testing data\n",
    "end=time.time()                                                                               \n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000 3997 3997\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(y_train),len(x_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005018711090087891\n"
     ]
    }
   ],
   "source": [
    "# creating numpy arrays of training and testing data as numpy arrays are faster\n",
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "keys=set(y_train)\n",
    "class_dict={}\n",
    "i=0\n",
    "for k in keys:\n",
    "    class_dict[k]=i       # replacing each class with a unique number as numpy arrays only contain numbers\n",
    "    i+=1\n",
    "l=[]\n",
    "start=time.time()\n",
    "for y in y_train:\n",
    "    l.append(class_dict[y])\n",
    "y_train=np.array(l)\n",
    "l=[]\n",
    "for y in y_test:\n",
    "    l.append(class_dict[y])\n",
    "y_test=np.array(l)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000 3997 3997\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(y_train),len(x_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function generates the dictonary which will be used in the predict function\n",
    "def fit(x_train,y_train,vocabulary):\n",
    "    dict={}\n",
    "    dict[\"total_data\"]=len(y_train) # total_data is the nummber of training data points\n",
    "    class_set=set(y_train)\n",
    "    for current_class in class_set:\n",
    "        dict[current_class]={}\n",
    "        current_class_data= (y_train==current_class)\n",
    "        current_class_data= x_train[current_class_data]    #total_count for each class is the number of training data \n",
    "        dict[current_class]['total_count']=len(current_class_data)       # points belonging to that class\n",
    "        idx=0\n",
    "        sum=0\n",
    "        for feature in vocabulary:\n",
    "            dict[current_class][feature]=current_class_data[:,idx].sum()\n",
    "            sum+=dict[current_class][feature]\n",
    "            idx+=1\n",
    "        dict[current_class]['total_words']=sum      # total_words for each class is the total number of words in        \n",
    "    return dict                                  # the training data for that class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it calculates the feature probability for one class\n",
    "def probability(dict,x):\n",
    "    ret_val=0\n",
    "    t=0\n",
    "    num_features=len(dict.keys())-2\n",
    "    den=dict['total_words']+ num_features   # num_features is laplace correction\n",
    "    idx1=list(dict.keys()).index('total_words')\n",
    "    idx2=list(dict.keys()).index('total_count')\n",
    "    num_array=list(dict.values()) \n",
    "    if idx1>idx2:\n",
    "        del num_array[idx1]\n",
    "        del num_array[idx2]\n",
    "    else:\n",
    "        del num_array[idx2]\n",
    "        del num_array[idx1]\n",
    "    start=time.time()\n",
    "    \n",
    "    num_array=np.array(num_array)\n",
    "    num_array=num_array+1   # 1 is laplace correction\n",
    "    num_array=num_array/den\n",
    "    array=np.log(num_array)\n",
    "    array=array*x    # multiplying the probability of each word with its frequency of occurence in that testing data pt\n",
    "    ret_val=array.sum()\n",
    "    end=time.time()\n",
    "    t+=end-start\n",
    "    return ret_val,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it predicts the class for one testing data point\n",
    "def predict_single_point(dict,x):\n",
    "    classes=dict.keys()\n",
    "    t=0\n",
    "    ret_val=-10\n",
    "    class_name=-100\n",
    "    check_first=True\n",
    "    log_total_data= np.log(dict['total_data'])\n",
    "    for curr_class in classes:    # calculate probability for all classes and select the class with max. probability \n",
    "        if curr_class=='total_data':\n",
    "            continue\n",
    "        prob_class= np.log(dict[curr_class]['total_count'])- log_total_data  # prior probability or class probability\n",
    "        prob_feature,temp=probability(dict[curr_class],x)    # feature probability\n",
    "        t+=temp\n",
    "        prob=prob_class+prob_feature\n",
    "        if check_first :\n",
    "            ret_val=prob\n",
    "            class_name=curr_class\n",
    "            check_first=False\n",
    "        if ret_val<prob:\n",
    "            ret_val=prob\n",
    "            class_name=curr_class\n",
    "    return class_name,t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it predicts the class for all 4000 testing data points\n",
    "def predict(dict,x_test):\n",
    "    y_pred=[]\n",
    "    t=0\n",
    "    for i in range(x_test.shape[0]):\n",
    "        y,temp=predict_single_point(dict,x_test[i,:])\n",
    "        y_pred.append(y)\n",
    "        t+=temp\n",
    "    print(t)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0304512977600098\n",
      "67.78162145614624\n",
      "108.31429195404053\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.66      0.74       200\n",
      "          1       0.75      0.93      0.83       200\n",
      "          2       0.81      0.81      0.81       200\n",
      "          3       0.89      0.95      0.92       200\n",
      "          4       0.96      0.86      0.91       200\n",
      "          5       0.90      0.55      0.68       200\n",
      "          6       0.64      0.81      0.71       200\n",
      "          7       0.70      0.69      0.70       200\n",
      "          8       0.87      0.70      0.78       200\n",
      "          9       0.97      0.96      0.96       200\n",
      "         10       0.81      0.92      0.86       200\n",
      "         11       0.95      0.91      0.93       200\n",
      "         12       0.63      0.66      0.64       200\n",
      "         13       0.89      0.93      0.90       200\n",
      "         14       0.76      0.90      0.82       200\n",
      "         15       0.94      0.91      0.92       200\n",
      "         16       0.92      0.93      0.92       200\n",
      "         17       0.96      0.96      0.96       200\n",
      "         18       0.95      1.00      0.98       197\n",
      "         19       0.78      0.74      0.76       200\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3997\n",
      "\n",
      "[[132   0   0  14   0   0   0   1   0   0  30   1  20   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0 185   0   0   0   1   6   1   0   0   0   0   0   0   7   0   0   0\n",
      "    0   0]\n",
      " [  4   1 162   9   1   1   4   8   0   0   0   1   0   1   5   2   1   0\n",
      "    0   0]\n",
      " [  3   1   0 190   0   1   0   1   0   0   1   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  2   5   5   0 173   0   0   3   0   2   0   0   0   3   2   0   3   2\n",
      "    0   0]\n",
      " [  0  12   2   0   0 109  42  13  13   0   0   0   0   0   8   0   1   0\n",
      "    0   0]\n",
      " [  0  18  10   0   0   2 162   1   0   0   0   0   0   0   4   1   1   0\n",
      "    0   1]\n",
      " [  0  13   5   0   1   3  12 139   7   0   0   7   0   0   9   3   1   0\n",
      "    0   0]\n",
      " [  0   5   2   0   1   3  17  24 141   0   0   0   0   0   2   2   3   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   0   0   0 192   0   0   0   1   3   0   1   2\n",
      "    0   0]\n",
      " [  2   0   0   0   0   0   1   0   0   0 184   0   8   5   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   4   0   0   0   2   4   1   0   1 182   0   1   3   1   1   0\n",
      "    0   0]\n",
      " [  7   0   0   0   0   0   0   1   0   0  10   0 131   1   0   1   0   0\n",
      "    9  40]\n",
      " [  1   0   3   0   1   0   0   1   1   0   0   0   0 185   4   0   3   1\n",
      "    0   0]\n",
      " [  0   4   4   0   0   0   9   0   0   0   1   0   0   1 179   0   0   2\n",
      "    0   0]\n",
      " [  7   2   1   0   2   0   0   2   0   0   0   0   0   4   0 181   0   0\n",
      "    0   1]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   5   8   0 186   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   1   0   0   0   4   0   0   0   0   1   0   0 193\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  197   0]\n",
      " [  1   0   1   0   0   0   0   0   0   0   0   0  45   1   0   1   2   0\n",
      "    1 148]]\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "dictionary= fit(x_train,y_train,vocabulary)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "y_pred=predict(dictionary,x_test)\n",
    "end1=time.time()\n",
    "print(end1-end)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1343269348144531\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.66      0.74       200\n",
      "          1       0.75      0.93      0.83       200\n",
      "          2       0.81      0.81      0.81       200\n",
      "          3       0.89      0.95      0.92       200\n",
      "          4       0.96      0.86      0.91       200\n",
      "          5       0.90      0.55      0.68       200\n",
      "          6       0.64      0.81      0.71       200\n",
      "          7       0.70      0.69      0.70       200\n",
      "          8       0.87      0.70      0.78       200\n",
      "          9       0.97      0.96      0.96       200\n",
      "         10       0.81      0.92      0.86       200\n",
      "         11       0.95      0.91      0.93       200\n",
      "         12       0.63      0.66      0.64       200\n",
      "         13       0.89      0.93      0.90       200\n",
      "         14       0.76      0.90      0.82       200\n",
      "         15       0.94      0.91      0.92       200\n",
      "         16       0.92      0.93      0.92       200\n",
      "         17       0.96      0.96      0.96       200\n",
      "         18       0.95      1.00      0.98       197\n",
      "         19       0.78      0.74      0.76       200\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3997\n",
      "\n",
      "[[132   0   0  14   0   0   0   1   0   0  30   1  20   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0 185   0   0   0   1   6   1   0   0   0   0   0   0   7   0   0   0\n",
      "    0   0]\n",
      " [  4   1 162   9   1   1   4   8   0   0   0   1   0   1   5   2   1   0\n",
      "    0   0]\n",
      " [  3   1   0 190   0   1   0   1   0   0   1   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  2   5   5   0 173   0   0   3   0   2   0   0   0   3   2   0   3   2\n",
      "    0   0]\n",
      " [  0  12   2   0   0 109  42  13  13   0   0   0   0   0   8   0   1   0\n",
      "    0   0]\n",
      " [  0  18  10   0   0   2 162   1   0   0   0   0   0   0   4   1   1   0\n",
      "    0   1]\n",
      " [  0  13   5   0   1   3  12 139   7   0   0   7   0   0   9   3   1   0\n",
      "    0   0]\n",
      " [  0   5   2   0   1   3  17  24 141   0   0   0   0   0   2   2   3   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   0   0   0 192   0   0   0   1   3   0   1   2\n",
      "    0   0]\n",
      " [  2   0   0   0   0   0   1   0   0   0 184   0   8   5   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   4   0   0   0   2   4   1   0   1 182   0   1   3   1   1   0\n",
      "    0   0]\n",
      " [  7   0   0   0   0   0   0   1   0   0  10   0 131   1   0   1   0   0\n",
      "    9  40]\n",
      " [  1   0   3   0   1   0   0   1   1   0   0   0   0 185   4   0   3   1\n",
      "    0   0]\n",
      " [  0   4   4   0   0   0   9   0   0   0   1   0   0   1 179   0   0   2\n",
      "    0   0]\n",
      " [  7   2   1   0   2   0   0   2   0   0   0   0   0   4   0 181   0   0\n",
      "    0   1]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   5   8   0 186   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   1   0   0   0   4   0   0   0   0   1   0   0 193\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  197   0]\n",
      " [  1   0   1   0   0   0   0   0   0   0   0   0  45   1   0   1   2   0\n",
      "    1 148]]\n",
      "0.838378784088\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.81      0.84       800\n",
      "          1       0.89      0.97      0.93       800\n",
      "          2       0.95      0.94      0.94       800\n",
      "          3       0.96      0.97      0.97       800\n",
      "          4       0.99      0.94      0.96       800\n",
      "          5       0.96      0.68      0.79       800\n",
      "          6       0.77      0.94      0.85       800\n",
      "          7       0.86      0.93      0.90       800\n",
      "          8       0.97      0.88      0.92       800\n",
      "          9       0.99      0.98      0.98       800\n",
      "         10       0.92      0.95      0.93       800\n",
      "         11       0.99      0.98      0.98       800\n",
      "         12       0.77      0.74      0.76       800\n",
      "         13       0.96      0.96      0.96       800\n",
      "         14       0.88      0.97      0.92       800\n",
      "         15       0.98      0.97      0.98       800\n",
      "         16       0.98      0.98      0.98       800\n",
      "         17       0.98      0.98      0.98       800\n",
      "         18       0.98      1.00      0.99       800\n",
      "         19       0.86      0.89      0.87       800\n",
      "\n",
      "avg / total       0.93      0.92      0.92     16000\n",
      "\n",
      "[[644   0   0  28   1   0   0   0   0   0  36   3  81   2   3   2   0   0\n",
      "    0   0]\n",
      " [  0 773   2   0   0   3  14   0   0   0   0   0   0   0   8   0   0   0\n",
      "    0   0]\n",
      " [  0   9 750   0   0   0  16   4   0   0   0   0   0   9  10   2   0   0\n",
      "    0   0]\n",
      " [ 14   0   1 778   0   0   0   0   0   0   2   0   3   0   1   0   0   0\n",
      "    1   0]\n",
      " [  1   6  10   2 749   0   4  10   0   2   0   0   0   1   7   3   4   1\n",
      "    0   0]\n",
      " [  0  27   3   0   0 544 148  40  17   0   0   0   0   1  19   1   0   0\n",
      "    0   0]\n",
      " [  0  24   2   0   0   2 756   8   0   0   0   0   0   0   7   0   1   0\n",
      "    0   0]\n",
      " [  0   9   2   0   0   5  19 747   6   0   0   1   0   1  10   0   0   0\n",
      "    0   0]\n",
      " [  0  15   4   0   2   7  13  42 707   0   0   0   0   0   8   1   1   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   0   0 782   0   0   0   2   2   1   4   8\n",
      "    0   0]\n",
      " [ 22   0   0   0   0   1   0   0   0   0 757   5  12   0   1   1   1   0\n",
      "    0   0]\n",
      " [  1   0   1   0   0   1   0   6   1   0   1 787   0   0   1   0   0   1\n",
      "    0   0]\n",
      " [ 42   0   0   3   1   1   0   1   0   0  24   0 593   1   1   0   0   0\n",
      "   13 120]\n",
      " [  1   1   7   0   0   0   0   1   0   0   0   0   0 767  16   0   4   3\n",
      "    0   0]\n",
      " [  0   2   6   0   0   2   7   0   0   0   0   0   0   8 774   1   0   0\n",
      "    0   0]\n",
      " [  1   3   3   1   1   2   0   7   0   0   1   1   0   0   4 775   0   1\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   7   6   0 786   0\n",
      "    0   0]\n",
      " [  2   0   0   0   0   1   0   0   0   6   2   0   0   1   3   0   1 784\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "  799   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   1  77   0   1   0   0   0\n",
      "    4 715]]\n",
      "0.9229375\n"
     ]
    }
   ],
   "source": [
    "# using the inbuilt multinomial naive bayes for text classification\n",
    "clf=MultinomialNB()\n",
    "start=time.time()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(clf.score(x_test,y_test))\n",
    "y_pred=clf.predict(x_train)\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(confusion_matrix(y_train,y_pred))\n",
    "print(clf.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# testing cell ################\n",
    "\n",
    "def cmp(t):\n",
    "    return t[1]\n",
    "\n",
    "d={'e':1,'a':4,'r':0,'b':10}\n",
    "type(d.items())\n",
    "sorted(d.items(),key = cmp)\n",
    "\n",
    "d=((1,'e'),(4,'a'),(0,'r'),(10,'b'))\n",
    "sorted(d,reverse=True)\n",
    "\n",
    "d=[(1,'abc'),(4,'azxc'),(0,'rwef'),(10,'bdc')]\n",
    "sorted(d,reverse=True)\n",
    "\n",
    "for k in range(10):\n",
    "    if k>5:\n",
    "        break\n",
    "    print(k)\n",
    "    \n",
    "#\"abcde\"-\"abc\"\n",
    "os.listdir('E:/machine_learning/attachments/csv/text_classification_data/20_newsgroups/20_newsgroups')[0]\n",
    "'sh' in 'yash'\n",
    "type(os.listdir(rootdir))\n",
    "a=np.array([[1,2],[2,3],[3,4]])\n",
    "a[1,:].shape\n",
    "\n",
    "d={'e':1,'a':4,'r':0,'b':10}\n",
    "list(d.keys()).index('a')\n",
    "len(d)\n",
    "\n",
    "l=[]\n",
    "l1=np.array([1,2,3])\n",
    "l.append(l1)\n",
    "l\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "x_train_trans=pca.fit_transform(x_train)\n",
    "x_test_trans=pca.transform(x_test)\n",
    "pca.explained_variance_\n",
    "curr_var=0\n",
    "k=0\n",
    "total=pca.explained_variance_.sum()\n",
    "while curr_var/total < 0.99:\n",
    "    curr_var+=pca.explained_variance_[k]\n",
    "    k+=1\n",
    "k=k-1\n",
    "k\n",
    "pca=PCA(n_components=k)\n",
    "x_train_pca=pca.fit_transform(x_train)\n",
    "x_test_pca=pca.transform(x_test)\n",
    "x_train=x_train_pca\n",
    "x_test=x_test_pca\n",
    "\n",
    "\n",
    "\n",
    "a=[1,1,1,0,1,0,0,2,2,1,2]\n",
    "Counter(a)\n",
    "Counter(a)[1]\n",
    "\n",
    "a=[1,2,1,3,4,5]\n",
    "b=[1,2,3,4,6,7]\n",
    "set(a)-set(b)\n",
    "\n",
    "a=[1,10,20,20,1,30]\n",
    "np.log(a)\n",
    "\n",
    "a=np.array([])\n",
    "#a.append(1)\n",
    "a\n",
    "\n",
    "a=np.array([2,4,6,8,10])\n",
    "b=np.array([2,4,6,8,5])\n",
    "a/b\n",
    "\n",
    "d={'e':1,'a':4,'r':0,'b':10}\n",
    "len(d.keys())\n",
    "\n",
    "a=[1,2,3,4]\n",
    "\n",
    "def check(d):\n",
    "    print(d)\n",
    "    del d['a']\n",
    "    print(d)\n",
    "    return\n",
    "\n",
    "d={1:{'a':1,'b':2},2:{'a':4,'b':8}}\n",
    "d1={}\n",
    "for k,v in d.items():\n",
    "    d1[k]=v\n",
    "\n",
    "print(d)\n",
    "print(d1)\n",
    "check(d1[1])\n",
    "print(d)\n",
    "print(d1)\n",
    "\n",
    "a=[1,2,3,4,5]\n",
    "del a[1,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
